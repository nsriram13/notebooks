{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim example on Simple Wikipedia\n",
    "\n",
    "Based on [this example](https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import copy\n",
    "import re\n",
    "import json\n",
    "import tarfile\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "# Uncomment to print Gensim log messages\n",
    "# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "# logging.root.level = logging.INFO\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Wikipedia data. The Simple Wikipedia corpus can be [downloaded here](https://dumps.wikimedia.org/simplewiki/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  April , text: len: 17304 , sample:  \"\\n'''April''' is the 4th month of the year, and com\"\n",
      "Title:  August , text: len: 9896 , sample:  \"\\n'''August''' (Aug.) is the 8th month of the year \"\n",
      "Title:  Art , text: len: 5289 , sample:  \"A painting by Renoir which is a work of art.\\n\\n'''A\"\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from gensim.utils import smart_open\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "\n",
    "DATA_PATH = './data/simplewiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "def iter_wiki():\n",
    "    for title, text, pageid in _extract_pages(smart_open(DATA_PATH)):\n",
    "        text = filter_wiki(text)\n",
    "        yield title, text\n",
    "\n",
    "test_examples = [(ti, te) for ti, te in itertools.islice(iter_wiki(), 3)]\n",
    "for title, text in test_examples:\n",
    "    print('Title: ', title, ', text: len:', len(text), ', sample: ', repr(text[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and create a Bag-Of-Words dictionary from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sample:  ['april', 'th', 'month', 'year', 'comes']\n",
      "Tokens sample:  ['august', 'aug', 'th', 'month', 'year']\n",
      "Tokens sample:  ['painting', 'renoir', 'work', 'art', 'art']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in gensim.utils.tokenize(text, lowercase=True, deacc=True) \n",
    "            if token not in STOPWORDS]\n",
    "\n",
    "for title, text in test_examples:\n",
    "    print('Tokens sample: ', tokenize(text)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 42s, sys: 570 ms, total: 3min 42s\n",
      "Wall time: 3min 42s\n",
      "id2word_wiki:  Dictionary(547548 unique tokens: ['april', 'th', 'month', 'year', 'comes']...)\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary (This wil take a couple of minutes)\n",
    "doc_stream = (tokenize(text) for title, text in iter_wiki())\n",
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "print('id2word_wiki: ', id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2word_wiki:  Dictionary(36692 unique tokens: ['april', 'th', 'month', 'year', 'comes']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_wiki.filter_extremes(no_below=20, no_above=0.1)\n",
    "print('id2word_wiki: ', id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April [(0, 226), (1, 2), (2, 9)] ['april', 'th', 'month']\n",
      "August [(1, 2), (2, 14), (3, 14)] ['th', 'month', 'year']\n",
      "Art [(29, 4), (70, 3), (77, 1)] ['making', 'new', 'including']\n"
     ]
    }
   ],
   "source": [
    "# Vectorize example\n",
    "for title, text in test_examples:\n",
    "    bow_test = id2word_wiki.doc2bow(tokenize(text))\n",
    "    print(title, bow_test[:3], [id2word_wiki[i] for i, _ in bow_test[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "april 226\n",
      "august 140\n",
      "art 41\n"
     ]
    }
   ],
   "source": [
    "# Example on how to find most common words\n",
    "for title, text in test_examples:\n",
    "    bow_test = id2word_wiki.doc2bow(tokenize(text))\n",
    "    most_index, most_count = max(bow_test, key=lambda t: t[1])\n",
    "    print(id2word_wiki[most_index], most_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min, sys: 1.09 s, total: 4min 1s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "# Serialise the corpus (takes a couple of minutes)\n",
    "wiki_corpus_gen = (id2word_wiki.doc2bow(tokenize(text)) for title, text in iter_wiki())\n",
    "%time gensim.corpora.MmCorpus.serialize('./data/wiki_bow.mm', wiki_corpus_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm_corpus:  MmCorpus(242253 documents, 36692 features, 9144383 non-zero entries)\n",
      "mm_corpus[0]:  [(0, 226.0), (1, 2.0), (2, 9.0), (3, 24.0), (4, 4.0), (5, 14.0), (6, 5.0)]\n"
     ]
    }
   ],
   "source": [
    "mm_corpus = gensim.corpora.MmCorpus('./data/wiki_bow.mm')\n",
    "print('mm_corpus: ', mm_corpus)\n",
    "print('mm_corpus[0]: ', mm_corpus[0][:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling\n",
    "\n",
    "With LDA and LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 1.18 s, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "# LDA topic-modelling on a subset of documents\n",
    "mm_corpus_subset = gensim.utils.ClippedCorpus(mm_corpus, 5000)\n",
    "%time lda_model = gensim.models.LdaModel(mm_corpus_subset, num_topics=10, id2word=id2word_wiki, passes=4)\n",
    "# Serialise the LDA model\n",
    "# lda_model.save('./data/lda_wiki.model')\n",
    "# Serialise corpus transformed to LDA space\n",
    "# %time gensim.corpora.MmCorpus.serialize('./data/wiki_lda.mm', lda_model[mm_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  0.011*\"hex\" + 0.011*\"rgb\" + 0.009*\"person\" + 0.008*\"deleted\" + 0.008*\"color\" + 0.008*\"utc\" + 0.008*\"english\"\n",
      "Topic 1:  0.016*\"january\" + 0.015*\"march\" + 0.013*\"february\" + 0.013*\"april\" + 0.012*\"day\" + 0.011*\"july\" + 0.011*\"december\"\n",
      "Topic 2:  0.044*\"b\" + 0.044*\"d\" + 0.040*\"american\" + 0.011*\"actor\" + 0.010*\"politician\" + 0.010*\"english\" + 0.009*\"actress\"\n",
      "Topic 3:  0.008*\"water\" + 0.008*\"jpg\" + 0.007*\"called\" + 0.006*\"like\" + 0.005*\"image\" + 0.005*\"usually\" + 0.004*\"different\"\n",
      "Topic 4:  0.015*\"tower\" + 0.013*\"u\" + 0.012*\"c\" + 0.010*\"transmission\" + 0.010*\"mast\" + 0.008*\"g\" + 0.008*\"m\"\n",
      "Topic 5:  0.013*\"city\" + 0.009*\"country\" + 0.008*\"south\" + 0.007*\"world\" + 0.007*\"united\" + 0.007*\"north\" + 0.007*\"state\"\n",
      "Topic 6:  0.010*\"city\" + 0.009*\"rural\" + 0.007*\"war\" + 0.005*\"germany\" + 0.005*\"called\" + 0.005*\"th\" + 0.005*\"world\"\n",
      "Topic 7:  0.017*\"music\" + 0.009*\"album\" + 0.007*\"band\" + 0.006*\"love\" + 0.005*\"song\" + 0.005*\"songs\" + 0.005*\"rock\"\n",
      "Topic 8:  0.008*\"called\" + 0.006*\"number\" + 0.006*\"use\" + 0.005*\"light\" + 0.005*\"different\" + 0.005*\"time\" + 0.005*\"example\"\n",
      "Topic 9:  0.006*\"best\" + 0.005*\"called\" + 0.005*\"movie\" + 0.005*\"jackson\" + 0.005*\"life\" + 0.004*\"award\" + 0.004*\"time\"\n"
     ]
    }
   ],
   "source": [
    "# Print a the most imortant words for some of the topics\n",
    "for i in range(lda_model.num_topics):\n",
    "    topic = lda_model.print_topic(i, topn=7)\n",
    "    print('Topic {}: '.format(i), topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April [(1, 0.96864864118566418), (5, 0.030853415799662484)]\n",
      "August [(1, 0.92873253424234059), (2, 0.032780138361494147), (5, 0.029970805995041965)]\n",
      "Art [(0, 0.097266361244568042), (3, 0.22744177535490454), (6, 0.12031737546691786), (7, 0.24843586564719863), (8, 0.27067622290004123), (9, 0.034907546686844718)]\n"
     ]
    }
   ],
   "source": [
    "for title, text in test_examples:\n",
    "    bow_test = id2word_wiki.doc2bow(tokenize(text))\n",
    "    print(title, lda_model[bow_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 98.4 ms, total: 19.7 s\n",
      "Wall time: 19.8 s\n",
      "CPU times: user 3min 9s, sys: 8.92 s, total: 3min 18s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF transformed LSI topic modelling\n",
    "# TF-IDF will take a couple of seconds on the full corpus\n",
    "%time tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_wiki)\n",
    "# Building LSI model on top of tf-idf will take a couple of minutes\n",
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_wiki, num_topics=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April [(0, 0.8347284958624097), (1, 0.0066557863597582825), (2, 0.04867220330799707), (3, 0.07974327438792456), (4, 0.018764337390088714)]\n",
      "August [(1, 0.010087312917778705), (2, 0.1147473468245084), (3, 0.07049965283991218), (5, 0.027609990322833237), (6, 0.015332692432403861)]\n",
      "Art [(29, 0.06500274952987092), (70, 0.030321423028594887), (77, 0.013980524606924807), (92, 0.07351729658992928), (98, 0.02186757327473417)]\n"
     ]
    }
   ],
   "source": [
    "# Print some of the TF-IDF transformations\n",
    "for title, text in test_examples:\n",
    "    bow_test = id2word_wiki.doc2bow(tokenize(text))\n",
    "    print(title, tfidf_model[bow_test][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics in LSI model:  200\n",
      "Topic 0:  0.455*\"commune\" + 0.400*\"department\" + 0.398*\"france\" + 0.308*\"region\" + 0.141*\"calais\" + 0.140*\"north\" + 0.140*\"aisne\"\n",
      "Topic 1:  0.424*\"utc\" + 0.406*\"discussion\" + 0.346*\"talk\" + 0.235*\"page\" + 0.231*\"delete\" + -0.158*\"commune\" + -0.136*\"department\"\n",
      "Topic 2:  0.319*\"league\" + 0.293*\"football\" + 0.237*\"city\" + 0.229*\"united\" + 0.217*\"states\" + 0.192*\"statistics\" + 0.186*\"j\"\n",
      "Topic 3:  -0.339*\"city\" + -0.326*\"states\" + 0.319*\"league\" + -0.306*\"united\" + 0.287*\"football\" + -0.253*\"county\" + 0.197*\"j\"\n",
      "Topic 4:  -0.940*\"template\" + -0.140*\"infobox\" + -0.126*\"data\" + -0.097*\"country\" + 0.073*\"discussion\" + 0.067*\"utc\" + 0.066*\"city\"\n"
     ]
    }
   ],
   "source": [
    "print('Number of topics in LSI model: ', lsi_model.num_topics)\n",
    "# Print a the most imortant words for some of the topics\n",
    "for i in range(5):\n",
    "    topic = lsi_model.print_topic(i, topn=7)\n",
    "    print('Topic {}: '.format(i), topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April [(0, 0.057208848408777505), (1, 0.069549132386538121), (2, 0.11163427860070518), (3, -0.046112950249986018), (4, 0.00066600030485395689)]\n",
      "August [(0, 0.040953372002199791), (1, 0.064658783492308636), (2, 0.083755512095660636), (3, -0.020348795777710454), (4, -0.0013647585516808462)]\n",
      "Art [(0, 0.020952291947944124), (1, 0.032866134424242216), (2, 0.038000092366583214), (3, -0.019156842554201348), (4, -0.008658660220001578)]\n"
     ]
    }
   ],
   "source": [
    "# Print some of the LSI transformations\n",
    "for title, text in test_examples:\n",
    "    bow_test = id2word_wiki.doc2bow(tokenize(text))\n",
    "    print(title, lsi_model[tfidf_model[bow_test]][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_vector:  [('forces', 1), ('physical', 1), ('structure', 1), ('mechanics', 1), ('study', 1), ('physics', 1), ('energy', 1), ('atoms', 1), ('universe', 1), ('waves', 1)]\n",
      "\n",
      "lda_vector:  [(8, 0.9181732909213518)]\n",
      "Most important LDA topic:  0.008*\"called\" + 0.006*\"number\" + 0.006*\"use\" + 0.005*\"light\" + 0.005*\"different\" + 0.005*\"time\" + 0.005*\"example\" + 0.005*\"energy\" + 0.005*\"like\" + 0.004*\"things\"\n",
      "\n",
      "Most important LSI topic:  0.227*\"mario\" + 0.159*\"zurich\" + 0.149*\"bundesliga\" + -0.139*\"republic\" + -0.137*\"ret\" + 0.137*\"super\" + 0.135*\"characters\" + -0.131*\"stadium\" + 0.130*\"e\" + -0.123*\"tropical\"\n"
     ]
    }
   ],
   "source": [
    "# Test on unseen text\n",
    "text = 'Physics is the study of energy, forces, mechanics, waves, and the structure of atoms and the physical universe.'\n",
    "\n",
    "# Transform to BOW\n",
    "bow_vector = id2word_wiki.doc2bow(tokenize(text))\n",
    "print('bow_vector: ', [(id2word_wiki[id], count) for id, count in bow_vector])\n",
    "print('')\n",
    "\n",
    "# transform into LDA space\n",
    "lda_vector = lda_model[bow_vector]\n",
    "print('lda_vector: ', lda_vector)\n",
    "print('Most important LDA topic: ', lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))\n",
    "print('')\n",
    "\n",
    "# Transform into the LSI space\n",
    "lsi_vector = lsi_model[tfidf_model[bow_vector]]\n",
    "print('Most important LSI topic: ', lsi_model.print_topic(max(lsi_vector, key=lambda item: abs(item[1]))[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
